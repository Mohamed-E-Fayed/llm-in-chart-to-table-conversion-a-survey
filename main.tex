\documentclass[
	%a4paper, % Use A4 paper size
	letterpaper, % Use US letter paper size
]{jdf}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{verbatim}
\addbibresource{references.bib}

\author{Mohamed Fayed, Kruthik Ravikanti, Bruce Walker}
\email{mohamed.fayed@gatech.edu, kravikanti3@gatech.edu, bruce.walker@psych.gatech.edu}
\title{Chart-to-Table Conversion: A Survey}

\begin{document}
%\lsstyle

\maketitle

\begin{abstract}
    Multimodal   Large Language Models (MLLMs) have demonstrated impressive visual capabilities in many Visual Question Answering (VQA) tasks.
    Chart-to-Table Conversion is the problem of extracting structured tabular data out of input images, enabling better accessibility and higher storage efficiency.
In this survey, we review the recent advancements in Chart-to-Table task, analyze existing datasets and benchmark some state-of-the-art models.
Our quantitative and qualitative analysis highlights strengths and limitations of current approaches in handling complex charts, preserving numerical precision and generalizing across various types of charts.
Our study reveals significant room for improvement in Chart-to-Table Conversion, emphasizing the need for more specialized and robust methods.
   \end{abstract}

   %\tableofcontents

\section{Introduction}\label{sect:intro}
Data visualization is an essential tool for communicating information efficiently, with charts and graphs playing a fundamental role in various domains, including finance, healthcare, and scientific research.
These visualizations are usually saved in form of images, and thus require extracting structured tabular data from chart images remains a challenging problem, particularly for automated systems.
This challenge is especially critical for accessibility, where individuals with visual impairments rely on machine-readable formats to interpret visual information.

Chart-to-Table is the task of extracting data points from an image of a chart into a table usually in markdown\cite{liu2022deplot,masry2024chartgemma}.
It was usually tackled via combination of rule-based tecniques, image processing, CNN-based deep learning architectures and optical character recognition (OCR).
These techniques struggle with diverse chart formats, occlusions and variations in visual styles.
More recently, this task was tackled via transformer-based models and Multimodal Large Language Models (MLLMs), which provide more robust solutions.

In this survey, we provide a comprehensive review of Chart-to-Table conversion techniques, covering benchmark datasets, model architectures, evaluation metrics, and existing challenges.
Furthermore, we assess the performance of several state-of-the-art transformers-based models, including MLLMs, on this task, highlighting their strengths and weaknesses.
Our findings suggest that while these models exhibit promising results, significant challenges remain, including generalization across different chart types, handling visual input effectively, and improving numerical fidelity.

Our main contributions are:
\begin{enumerate}
     \item Survey recent advancements in Chart-to-Table task; to the best of our knowledge, this is the first survey focusing on Chart-to-Table Conversion,
     \item Conduct a comprehensive qualitative and quantitative analysis on various benchmarks covering various common types of charts, and
     \item highlight strengths and limitations  of those models, emphasizing on rooms for improvement in tackling Chart-to-Table Conversion.
       \end{enumerate}

\section{Methodology}\label{sect:methodology}
\subsection{Paper Selection}\label{ssect:paper-selection}
Our goal is to select a diverse set of papers to illustrate the landscape of the task.
We have searched for any chart related effort because Chart-to-Table might be included within a larger scope of chart related tasks.
We considered latest research papers found in Google Scholar up to April 2025.
\subsection{Datasets}\label{ssect:datasets}
Many datasets were introduced for various chart related tasks, with no special attention to Chart-to-Table Conversion.
After investigation, we found that some datasets include chart images with their corresponding data points.
In our analysis, we focus on reporting scores on two of those testsets: ICPR22~\cite{rousseau2023pattern} and PlotQA~\cite{methani2020plotqa}.

\paragraph{ICPR22 testset~\cite{rousseau2023pattern}} was gathered from research papers published on PubMed Central website.\footnote{\href{https://pmc.ncbi.nlm.nih.gov}{https://pmc.ncbi.nlm.nih.gov}}
Those publications are in biomedical and life sciences domains.
It contains 443 charts splitted into 5 types: Line Charts, Horizontal and Vertical Bar Charts, Scatter Plot and Vertical Box Plot.
It represents a source of real-world generated charts, as they were generated by a large set of researchers.

\paragraph{PlotQA~\cite{methani2020plotqa}} was made by scrapping data from various online sources, such as World Bank and Open Data, generate plots out of these data points, and ask annotators questions about those provided charts.
In our work, we focus on the data points used in constructing the charts only.
Its test set contains 33657 charts divided equally among dotted line charts, line charts, and vertical and horizontal bar charts.
We hypothesized that there is no need to test on all $33k$ charts, so we performed comparison among scores for many sample sizes.
Table~\ref{tab:plotqa-scores-for-various-sizes} shows that it is enough to compute scores on a sample of $4k$ randomly selected charts.

\paragraph{Test-Sample:} For the sake of qualitative analysis, we selected a set of charts from PlotQA and ICPR22 testsets.
We randomly selected from each kind of chart in PlotQA.
However, for ICPR22, we selected charts from various data sizes, i.e. of various sizes of json files.
We noticed that larger file sizes are directly correlated to larger count of data points of corresponding charts.
%TODO: add link to download those files.

\subsection{Models}\label{ssect:models}
For our study, we tested the following models in zero-shot setting:
\begin{itemize}
  \item Gemini 1.5 Flash~\cite{team2024gemini}: A general purpose lightweight MLLM, which is the largest in this list.
     \item ChartGemma~\cite{masry2024chartgemma}: A specialized model in chart summarization, question answering and reasoning about charts.
       It utilizes PaliGemma 4B~\cite{beyer2024paligemma} as its backbone, and was tuned on Visual Chart Instructions dataset.
     \item DePlot~\cite{liu2022deplot}: A 282M parameters Vision Encoder-Text Decoder transformer architecture.
         It was trained on Chart-to-Table Conversion task.
       \end{itemize}
       \subsection{Prompts}\label{ssect:prompts}
       For DePlot, we used the prompt recommended in its Hugging Face Model Card: "Generate underlying data table of the figure below:".
       For ChartGemma, we used a simple prompt of "program of thought: extract data points and formulate them into markdown table.
    Note: all data points should be either integer or float.
    Do not write percentages as '56%', but write them as '0.5"

    For Gemini 1.5 Flash, we used a longer prompt, and if its output is not parse-able into json object, we prompt it again to fix its format.
    The main prompt is: 
\begin{verbatim}
    "System Message:     You are an EXPERT in scientific visualizations analysis and chart understanding and description.
    You should help people with disabilities in giving an overview of the chart and converting the chart into its original data point form.
        prompt: For the following image, tell me the name of x and y axes, and the values of the each axis.
        The output should be in format of 
        {x_axis_name: x_axis_vavlues_list, y_xis_name: y_axis_values_list}"
        \end{verbatim}

\subsection{Evaluation}\label{ssect:method-eval}
To compare among those models, we compute Relative Mapping Similarity (RMS)~\cite{liu2022deplot} and do qualitative analysis on our Test-Sample testset.

\section{Chart-to-Table in Literature}\label{sect:literature}
There has been a lot of work related to chart analysis in multiple dimensions.
Some researchers focused on converting charts to tables (Chart-to-Table Conversion).
Others concentrated on Chart Question Answering and Chart Summarization.
In this work, we focus on Chart-to-Table Conversion, which requires tackling Optical Character Recognition (OCR) and comprehensive Visual Understanding.
In this section, we review related work in Traditional Computer Vision, Deep Learning and Multimodal Large Language Models and how they influenced the evolution of this task.

\subsection{Computer Vision and Deep Learning}\label{ssect:cv-and-dl}
    It all begins with computer vision (CV)-based methods.
    Early attempts to extract tabular data from charts utilized traditional image processing techniques like segmentation and edge detection that could identify visual elements like bars, axes, and legends~\cite{sreevalsan2021tensor}.
    These approaches were effective to some extent, but they struggled with the many variations found in chart styles and occlusions.
    Furthermore, rule-based approaches were often unsuccessful due to complex charts that had overlapping elements or non-standard layouts~\cite{poco2017reverse}.
    As a result, researchers pursued more robust techniques that could generalize better across diverse chart designs.

    To deal with generalization problem, manny efforts were put towards integrating deep learning solutions.
    Systems such as ChartSense~\cite{jung2017chartsense} and ChartOCR~\cite{luo2021chartocr} utilized convolutional neural networks (CNNs) to classify types of charts and extract available data with higher accuracies.
    For instance, ChartOCR combined rule-based methods with deep CNNs to balance generalization and accuracy, achieving strong performance across multiple chart types~\cite{luo2021chartocr}.
    However, these methods needed extensive labeled datasets and lacked generalization across different chart formats.
    %Although CNNs did improve feature extraction, they still struggled with tasks demanding contextual understanding, such as contrasting similar visual elements.
    %To resolve this, researchers started to integrate transformers and multimodal models which allowed for better alignment between textual and visual elements of charts.

    There were many efforts towards handling specific types of charts using either Computer Vision or Deep Learning tecniques.
    ChartParser~\cite{kumar2022chartparser} was a pipeline that focuses on extracting data points for various kinds of bar charts.
    They build rule based algorithm using OCR and image processing techniques.
    Another group of researchers~\cite{li2022chart} focused on extracting underlying data points for camera photos of line charts using custom architecture based on Faster-RCNN~\cite{ren2015faster}.
    They gathered synthetic and real datasets for line charts to improve the performance of their model.
    Their approach shows promising direction in utilizing synthetic data in improving models on real world test images.
    This direction of research highlights challenges of each kind of chart, but it has many disconnected componenets and their results are not necessarily comparable.

    \subsection{Pretraining and Fine-tuning}\label{ssect:pretraining-and-fine-tuning}
After the introduction of transformers~\cite{vaswani2017attention}, Pretraining and Fine-tuning paradigm caught researchers attention.
~\cite{masry2023unichart} gathered a large dataset of 6.9M questions and charts, and used it to pre-train UniChart model.
Another group of researchers~\cite{liu2022matcha} introduced Matcha model, which was a fine-tuned version of Pix2Struct~\cite{lee2023pix2struct} on many tasks including Chart-to-Table.
Later on,~\cite{liu2022deplot} continued pre-training it on Chart-to-Table only to create DePlot.
They forwarded the generated table, human query and an example to FlanPaLM 540B~\cite{chung2024scaling} to answer complex queries.
~\cite{cheng2023chartreader} took a different approach by training a transformers based chart component detection and combine it with extended pretrained T5~\cite{raffel2020exploring} or TaPas~\cite{herzig2020tapas} models.

\subsection{Multimodal Large Language Models (MLLMs)}\label{ssect:mllm}
MLLMs have made significant progress in many tasks and Chart-to-Table was no exception.
One direction is to utilize general domain LLMs without any tuning.
This direction includes Prompting and Retrieval Augmented Generation techniques~\cite{cao2024graphinsight, voigt2023vist5}, which improve the capabilities of general LLMs on chart related tasks.
%TODO: add drawbacks to RAG in CTT

Another direction is to fine-tune LLMs on Chart-specific Instruction following datasets.
~\cite{masry2024chartinstruct} introduced an instruction-following dataset and instruction tuned both LLaMA 2 7B~\cite{touvron2023llama} and Flan-T5 XL 3B~\cite{chung2024scaling} on it.
This instruction tuning strategy proved to make a generalized model that can handle unseen chart related tasks.
Similarly,~\cite{masry2024chartgemma} fine-tuned PaliGemma~\cite{beyer2024paligemma} to create a 3B ChartGemma.
Another key distinguishing contribution was the method of instruction-following data generation.
They generated for predefined tasks, such as Chain of Thought and Chart-to-Tables in form of markdown, and open-ended tasks, such as justifying trends in charts and describing visual elements.

A unique direction is chart-to-code generation, which aims to provide lossless representations of charts.
Matcha~\cite{liu2022matcha} was the first model to include Chart-to-Code within its training tasks.
ChartCoder~\cite{zhao2025chartcoder} is a new model that translates chart images into executable Python code (e.g., using matplotlib).
Trained on the newly released Chart2Code-160k dataset containing 160k chart-code pairs across 27 chart types, ChartCoder uses a Code LLM backbone and a Snippet-of-Thought prompting method for step-by-step generation.
While intended for re-rendering, the output code preserves all chart data, making it a viable route for Chart-to-Table extraction through code interpretation.

Another recent model, ChartCitor~\cite{goswami2025chartcitor}, focuses on improving how chart question-answering models support their answers with evidence. Instead of just generating answers.
ChartCitor uses a group of LLM agents to first extract a table from a chart, then break the answer into smaller claims, and finally find the exact table cells that support those claims.
It also connects these cells back to specific parts of the chart using object detection and visual prompts.
ChartCitor does not aim to extract full tables, it introduces a useful method for linking visual data to structured output and improves transparency in chart understanding tasks, which makes it a good candidate for verifying correctness of generated tables.

\subsection{Chart-to-Table Evaluation}\label{ssect:task-evaluation}
There has been many metrics for evaluating Chart-to-Table conversion.
In~\cite{luo2021chartocr}, they introduced different metric for each kind of chart:
\begin{enumerate}
         \item For Bar Charts, defined a custom distance function for pairwise point comparison and find minimum cost between prediction and ground truth,
         \item For Line charts, it is handled as continuous similarity problem.
             For each predicted line, it computes the pointwise error between it and each ground truth line, and choose the minimal value.
         \item For Pie Charts, they consider its scoring as sequence matching problem, thus solved using dynamic programming.
              \end{enumerate}
Relative Number Similarity Score~\cite{masry2022chartqa}, also known as Relaxed Accuracy Measure, is to compute highest accuracy of generated numbers relative to ground truth.
However, this evaluation approach has two main drawbacks:
\begin{enumerate}
         \item It does not consider the position of numbers within the table, and
         \item It ignores textual errors.
              \end{enumerate}
To overcome those limitations, Relative Mapping Similarity (RMS)~\cite{liu2022deplot} computes edit distance between columns names, compute accuracy of values within columns of least edit distances and compute F1-score.
\footnote{our RMS implementation can be found \href{https://github.gatech.edu/mfayed8/GIE-metrics}{here}.}
To learn more about the algorithm, please check appendix~\ref{app:metrics}

\section{Results and Discussion}\label{sect:results}
\subsection{Scores}\label{ssect:rms}
In this section, we report the scores for each model.
While testing them, we noticed that they have issues in generating markdowns/jsons that can be correctly parsed by pandas.
So, we also report Success Rate (SR) for each model, such that $Success Rate (SR) = count of correctly parsed charts/count of predicted charts within the testset by selected model$.

We reported the results on a subset of PlotQA charts and all of ICPR22.
We were able to generate around 7000 charts from PlotQA.
We hypothesized that the size of 33k test images is very large and we should not find significant difference between scores on 3k and 33k.
To validate this hypothesis, we computed scores for 3k, 4k, 5k and 6k charts to see whether there is a significant need to infere on all 33k images.
Additionally, we computed the scores on the entire testset using DePlot model.
As shown in table~\ref{tab:plotqa-scores-for-various-sizes}, there are small differences across samples of 4k, 5k and k relative to variations among scores of all models.
This means that a sample of 4k is sufficient to provide a reliable estimate to model's performance.
\input{plotqa-scores-for-various-sizes}

\subsection{Discussion}\label{ssect:discussion}
\input{scores}
Fianl scores are shown in table~\ref{tab:final-scores}.
Gemini 1.5 Flash achieved the highest Success Rate (SR), indicating its strong ability to produce well-formatted and parse-able outputs.
However, DePlot outperformed all models in terms of Relative Mapping Similarity (RMS), showcasing its superior alignment with the ground truth table values.
On the other hand, ChartGemma recorded the lowest performance in both SR and RMS.

The relatively high SR of Gemini 1.5 Flash suggests that a powerful text decoder plays a crucial role in generating structured and clean outputs.
Meanwhile, DePlot’s strong RMS score—despite being a smaller, specialized model—underscores the value of task-specific architectural tuning.
These results suggest that instruction fine-tuning (as in ChartGemma) may not be sufficient to achieve high-quality Chart-to-Table conversions without deeper task alignment and robust visual reasoning.

The low scores of all models in ICPR22 testset indicate that it is challenging testset.
Upon manual inspection, we identified many contributors to low RMS scores.
One major issue is the absence of labels on many line charts.
Many columns in tables of the testset contain "unnamed data series 0", which has 1 normalized edit distance with whatever models generate as columns names, thus 0 RMS score occurs frequently.
In a future work, we would also take into account computing RMS both with and without considering normalized edit distance.
Another reason for those low scores is the failure of all models in properly extracting data from scatter plots and vertical box plots, as illustrated in section~\ref{sssect:scatter} and section~\ref{sssect:box} respectively.

\subsection{Text Recognition}\label{ssect:qualitative-text-recognition}
As per the analyzed sample, we found a single error in DePlot's output in  recognizing name of second row in table 28308 in PlotQA.
Other than that, all text was correctly recognized by all models.
However, tables~\ref{tab:deplot-plotqa-line-21673}, ~\ref{tab:chartgemma-plotqa-line-21673} and~\ref{tab:gemini-plotqa-line-21673} show models' tendency to labelize even if there are no labels in the input image.
\footnote{the prediction of Gemini and Ground Truth have no labels for x-axis, but ChartGemma made years as labels.}
\footnote{In some cases, the ground truth is mislabelled. The reference has no values for x-axis, but the image includes them as in~\ref{fig:plotqa-line-20049}.}

\subsection{Values Extraction}\label{ssect:values-extraction}
For PlotQA and ICPR22 samples, it is frequent to find errors like:
\begin{enumerate}
     \item rounding errors, e.g. $15.42->15$ and $15.6->15$.
     \item Precision Errors: we have noticed that the models can not predict more than 3 digits for Gemini 1.5 Flash and ChartGemma or 4 digits for DePlot per each value, e.g. $126765000.0->156000000$.
     \item In case of near values, e.g. $24.18, 24.09$, there might be some errors, e.g. predicting $23$ instead of $24$.
       That kind of error may result in changing trend, e.g. steady performance may seam as decreasing.
       \footnote{It is worth noting that we have not seen cases where increasing is replaced by decreasing trends or vice versa by any model in our Test-Sample.}
       \item All models can differentiate outputs based on scale, e.g. $156000000 \& 50.2$ for instance.
all models sometimes mispredict scale, e.g. table~\ref{tab:chartgemma-plotqa-line-21673} where ChartGemma returned values multiplied by $10$ relative to ground truth.
However, DePlot was found to be the most robust against this error.
       \item Occasionally, ChartGemma and Gemini 1.5 Flash swap two columns as shown in table~\ref{tab:gemini-plotqa-line-18806}.
         As a result, RMS score is significantly lower (f1=$34\%$) than its fixed version (f1=$83\%$).
       \end{enumerate}

       In the following subsubsections, we illustrate issues related to each kind of graphs.

\subsubsection{Bar Charts}\label{sssect:bar-errors}
\begin{enumerate}
    \item All models are good in extracting data points of Horizontal and Vertical bar charts.
        However, DePlot seams better in precision and in extracting negative points as in Figure~\ref{fig:plotqa-vbar-32640}.
       \end{enumerate}
       \begin{figure}
            \includegraphics{test-sample/plotqa/images/vertical-bar/32640.png}
            \caption{PlotQA Vertical Bar no. 32640.}
            \label{fig:plotqa-vbar-32640}
             \end{figure}
             \input{test-sample/plotqa/predictions/gemini-1.5-flash/1st_prompt/vertical-bar/32640.tex}
             \input{test-sample/plotqa/predictions/ahmed-masry/chartgemma/initial_prompt/vertical-bar/32640.tex}
             \input{ test-sample/plotqa/predictions/deplot/vertical-bar/32640.tex}
       \subsubsection{Line Charts}\label{sssect:line-errors}
       \begin{enumerate}
  \item Tables~\ref{tab:deplot-plotqa-line-21673}, ~\ref{tab:chartgemma-plotqa-line-21673} and~\ref{tab:gemini-plotqa-line-21673} show that both models are very good in extracting data points from line charts.
     \item There are some graphs, like~\ref{fig:icpr22-line-6339}, the Gemini API just fails with no clear response message.
       However, it is suspected that the large number of data points might be the reason.
               It is noticable for the same chart to cause DePlot to repeat some rows as shown in table~\ref{tab:deplot-icpr22-line-6339}.
     \item Table~\ref{tab:chartgemma-plotqa-line-20049} shows that ChartGemma may fail in extracting data points from slightly complex graphs.
       It fails in both extracting correct values as well as mapping them to the correct label.
              \end{enumerate}
       Tables~\ref{tab:gemini-plotqa-vbar-25905} and~\ref{tab:chartgemma-plotqa-vbar-25905} include Gemini 1.5 Flash and ChartGemma predictions for figure~\ref{fig:plotqa-vbar-25905} respectively.
\begin{figure}
   \includegraphics{test-sample/plotqa/images/vertical-bar/25905.png}
   \caption{Vertical Bar Chart example from PlotQA testset.}\label{fig:plotqa-vbar-25905}
   \end{figure}
\input{test-sample/plotqa/predictions/gemini-1.5-flash/1st_prompt/vertical-bar/25905.tex}
\input{test-sample/plotqa/predictions/ahmed-masry/chartgemma/initial_prompt/vertical-bar/25905.tex}
\input{test-sample/plotqa/predictions/deplot/vertical-bar/25905.tex}

\begin{figure}
   \includegraphics{test-sample/icpr22/images/line/PMC6339093___05.jpg}
   \caption{Example for charts that causes the API to fail.}
   \label{fig:icpr22-line-6339}
   \end{figure}
   \input{test-sample/icpr22/predictions/deplot/line/PMC6339093___05.tex}
\begin{figure}
   \includegraphics{test-sample/icpr22/images/line/PMC5882956___1_HTML.jpg}
   \caption{A good example for graph in the wild that causes Gemini 1.5 Flash to fail.}
   \label{fig:icpr22-line-5882}
   \end{figure}
\begin{figure}
   \includegraphics{test-sample/plotqa/images/line/21673.png}
   \caption{Example for Line Chart from PlotQA testset \# 21673 about Portfolio Investment}
   \label{fig:plotqa-line-21673}
   \end{figure}
\input{test-sample/plotqa/jsons/line/21673.tex}
\input{test-sample/plotqa/predictions/gemini-1.5-flash/1st_prompt/line/21673.tex}
\input{test-sample/plotqa/predictions/ahmed-masry/chartgemma/initial_prompt/line/21673.tex}
\input{test-sample/plotqa/predictions/deplot/line/21673.tex}

\begin{figure}
   \includegraphics{test-sample/plotqa/images/line/20049-4-lines.png}
   \caption{PlotQA \# 20049: Line chart containing 4 lines.}\label{fig:plotqa-line-20049}
   \end{figure}
   \input{test-sample/plotqa/jsons/line/20049.tex}
   \input{test-sample/plotqa/predictions/gemini-1.5-flash/1st_prompt/line/20049.tex}
   \input{test-sample/plotqa/predictions/ahmed-masry/chartgemma/initial_prompt/line/20049-4-lines.tex}

\input{test-sample/plotqa/predictions/gemini-1.5-flash/1st_prompt/line/18806.tex}
\subsubsection{Scatter Plots}\label{sssect:scatter}
\begin{enumerate}
         \item There is no model capable of handling scatter plots correctly!
          \end{enumerate}
          \begin{figure}
               \includegraphics{test-sample/icpr22/images/scatter/PMC2279396___g002.jpg}
               \caption{Scatter Plot from ICPR22 testset \#2279}
               \label{fig:icpr22-scatter-2279}
                \end{figure}
\begin{verbatim}
Ground Truth csv:
\input{test-sample/icpr22/csvs/scatter/PMC2279396___g002.csv}
DePlot output csv:
\input{test-sample/icpr22/predictions/deplot/scatter/PMC2279396___g002.csv}
Gemini 1.5 Flash output json:
\input{test-sample/icpr22/predictions/gemini-1.5-flash/1st_prompt/scatter/PMC2279396___g002.csv}
ChartGemma output text:
\input{test-sample/icpr22/predictions/ahmed-masry/chartgemma/initial_prompt/scatter/PMC2279396___g002.md}
\end{verbatim}
\subsubsection{Box Plots}\label{sssect:box}
\begin{enumerate}
         \item Gemini 1.5 Flash was the only model to correctly extract data points from vertical box plots.
             However, comparing Gemini 1.5 Flash's output to ground truth table needs custom logic because some graphs have more than one data series.
              \end{enumerate}
              \begin{figure}
                   \includegraphics{test-sample/icpr22/images/vertical-box/PMC1855992___g001.jpg}
                   \caption{Vertical Box Plot from ICPR22 testset \#1855}
                   \label{fig:icpr22-box-185}
                    \end{figure}
               \begin{verbatim}
               Ground Truth csv:
               \input{test-sample/icpr22/csvs/vertical-box/PMC1855992___g001.csv}
               DePlot prediction:
               \input{test-sample/icpr22/predictions/deplot/vertical-box/PMC1855992___g001.csv}
               Gemini 1.5 Flash prediction:
               \input{cat  test-sample/icpr22/predictions/gemini-1.5-flash/1st_prompt/vertical-box/PMC1855992___g001.json}
               ChartGemma's  output markdown:
               \input{test-sample/icpr22/predictions/ahmed-masry/chartgemma/initial_prompt/vertical-box/PMC1855992___g001.md}
               \end{verbatim}

\section{Conclusion and Recommendations}\label{sect:conclusion}
In conclusion, our study provides a comprehensive evaluation of the performance of various models on chart-to-table Conversion.
Our results show that while some models excel in certain types of charts, none of them perform evenly across all chart types.
This highlights the complexity and diversity of chart-to-table tasks, which requires specialized models or very capable large general models.

Our findings have important implications for researchers and practitioners working on data extraction and analysis.
They suggest that future research should focus on developing more flexible models that can handle a wide range  of chart types and styles.
Additionally, our results emphasize the need for larger and more diverse datasets to train and evaluate these models.

Overall, our study contributes to a deeper understanding of the challenges and opportunities in chart-to-table tasks and provides a foundation for future research in this area. 
We hope that our findings will fosters further innovation and improvement in data extraction and analysis.

\section{Limitations}\label{sect:limitations}
One limitation of our analysis is testing pre-trained models exclusively and did not test models like ChartOCR.
These models are not directly comparable and lack existing public weights and implementations.
This aspect has been deferred to future work.

\section{Acknowledgments}
This research was supported in part through research cyberinfrastructure resources and services provided by the Partnership for an Advanced Computing Environment (PACE)~\cite{pace2017partnership} at the Georgia Institute of Technology, Atlanta, Georgia, USA.

\section{References}
\printbibliography[heading=none]

\appendix
\section{Evaluation Metrics}\label{app:metrics}
\subsection{Relative Mapping Similarity (RMS)}\label{app:rms-algorithm}
\begin{algorithm}
    \begin{algorithmic}[1]
 \Require $prp$ (Predicted Table), $t$ (Ground Truth Table)
        \Ensure $rms\_precision, rms\_recall, rms\_f1$

        Compute Normalized Edit Distance between $p_r || p_c$ and $t_r || t_c$
    , where $||$ is concatenation operator.
        Compute pairwise similarities matrix
        make binarized similarities matrix by inserting 1 in place of highest similarities and zeros otherwise.
        \For $p_i$ in p.values:
        \For $t_j$ in t.values:
        $d_{theta_{ij}} = min(1, \frac{|p_i - t_j|}{|t_j|})$
        $d_{tao_{theta_{ij}}} = (1-Normalized Edit Distance) * (1-d_{theta})$
        \EndFor
        \EndFor
        $RMS_{precision} = \sum_{i} \sum_j d_{tao_{theta_{ij}}} / len(p)$
        $RMS_{recall} = \sum_{i} \sum_j d_{tao_{theta_{ij}}} / len(t)$
        $RMS_{f1} = 2 * \frac{RMS_{precision} * RMS_{recall}}{RMS_{precision} + RMS_{recall}}$
    \end{algorithmic}
\end{algorithm}
% \subsection{ChartOCR Bar Chart Metric}
% \subsection{ChartOCR Line Chart Metric}
% \subsection{ChartOCR Pie Chart Metric}
\subsection{metrics introduced in ChartOCR}\label{ssect:chartocr-metrics}
In this subsection, we illustrate metrics introduced by~\cite{luo2021chartocr}.
\subsubsection{Bar Chart Metric}

Given a predicted bounding box $p = [x_p, y_p, w_p, h_p]$ and a ground truth box $g = [x_g, y_g, w_g, h_g]$, the distance function $D(p, g)$ is defined as:

\[
D(p, g) = \min\left(1, \left\lvert \frac{x_p - x_g}{w_g} \right\rvert + \left\lvert \frac{y_p - y_g}{h_g} \right\rvert + \left\lvert \frac{h_p - h_g}{h_g} \right\rvert\right)
\]

Width $w$ is ignored because it does not affect chart reading.

Construct a pairwise cost matrix $C$ between predicted and ground truth boxes, then solve a minimum cost assignment problem using the Hungarian algorithm. The final score is:

\[
\text{Score} = 1 - \frac{\text{min cost}}{K}, \quad \text{where } K = \max(N, M)
\]

Here $N$ and $M$ are the number of predicted and ground truth boxes respectively.

\subsubsection{Line Chart Metric}

Let $P = \{(x_1, y_1), \ldots, (x_N, y_N)\}$ be the predicted points and $G = \{(u_1, v_1), \ldots, (u_M, v_M)\}$ be the ground truth. The recall is defined as:

\[
\text{Rec}(P, G) = \frac{1}{u_M - u_1} \sum_{i=1}^{M} \left(1 - \text{Err}(v_i, u_i, P)\right) \cdot \text{Intv}(i, G)
\]

Where:

\[
\text{Err}(v_i, u_i, P) = \min\left(1, \left\lvert \frac{v_i - I(P, u_i)}{v_i} \right\rvert\right)
\]

$I(P, u_i)$ is the interpolated value of $P$ at $x = u_i$. $\text{Intv}(i, G)$ is the interval weight:

\[
\text{Intv}(i, G) =
\begin{cases}
\frac{u_{i+1} - u_i}{2} & \text{if } i = 1 \\
\frac{u_i - u_{i-1}}{2} & \text{if } i = M \\
\frac{u_{i+1} - u_{i-1}}{2} & \text{otherwise}
\end{cases}
\]

Precision is defined similarly. The final score is the F1-score:

\[
F1 = \frac{2 \cdot \text{Prec} \cdot \text{Rec}}{\text{Prec} + \text{Rec}}
\]

\subsubsection{Pie Chart Metric}

Let $P = [x_1, \dots, x_N]$ and $G = [y_1, \dots, y_M]$ be the predicted and ground-truth value sequences in clockwise order. The match score is computed via dynamic programming:

\[
\text{score}(i,j) = \max\left(
\text{score}(i{-}1, j),
\text{score}(i, j{-}1),
\text{score}(i{-}1, j{-}1) + 1 - \left\lvert \frac{x_i - y_j}{y_j} \right\rvert
\right)
\]

with $\text{score}(0, j) = \text{score}(i, 0) = 0$ for all $i, j$. The final normalized matching score is:

\[
\text{Score} = \frac{\text{score}(N, M)}{M}
\]


\end{document}
