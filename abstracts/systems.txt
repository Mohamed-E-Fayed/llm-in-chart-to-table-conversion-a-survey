ChartSense: Interactive Data Extraction from Chart Images
ChartDetective: Easy and Accurate Interactive Data Extraction from Complex Vector Charts
Damien Masson, Sylvain Malacria, Daniel Vogel, Edward Lank, GÃ©ry Casiez 
Extracting underlying data from rasterized charts is tedious and in- accurate; values might be partially occluded or hard to distinguish, and the quality of the image limits the precision of the data being recovered. To address these issues, we introduce a semi-automatic system leveraging vector charts to extract the underlying data eas- ily and accurately. The system is designed to make the most of vector information by relying on a drag-and-drop interface com- bined with selection, filtering, and previsualization features. A user study showed that participants spent less than 4 minutes to accu- rately recover data from charts published at CHI with diverse styles, thousands of data points, a combination of different encodings, and

Daekyoung Jung1 Wonjae Kim1 Hyunjoo Song1 Jeong-in Hwang1 Bongshin Lee2 Bohyoung Kim3 Jinwook Seo1 1Seoul National University, Seoul, Republic of Korea 2Microsoft Research, Redmond, WA, USA 3Hankuk University of Foreign Studies, Yongin-si, Republic of Korea {rati, wjkim, hjsong, jihwang}@hcil.snu.ac.kr bongshin@microsoft.com bkim@hufs.ac.kr jseo@snu.ac.kr
Charts are commonly used to present data in digital documents such as web pages, research papers, or presentation slides. When the underlying data is not available, it is necessary to extract the data from a chart image to utilize the data for further analysis or improve the chart for more accurate perception. In this paper, we present ChartSense, an interactive chart data extraction system. ChartSense first determines the chart type of a given chart image using a deep learning based classifier, and then extracts underlying data from the chart image using semi-automatic, interactive extraction algorithms optimized for each chart type. To evaluate chart type classification accuracy, we compared ChartSense with ReVision, a system with the state-of-the-art chart type classifier. We found that ChartSense was more accurate than ReVision. In addition, to evaluate data extraction performance, we conducted a user study, comparing ChartSense with WebPlotDigitizer, one of the most effective chart data extraction tools among publicly accessible ones. Our results showed that ChartSense was better than WebPlotDigitizer in terms of task completion time, error rate, and subjective preference.

SeeChart: Enabling Accessible Visualizations Through Interactive Natural Language Interface For People with Visual Impairments
MD ZUBAIR IBNE ALAM, York University, Canada SHEHNAZ ISLAM, York University, Canada ENAMUL HOQUE, York University, Canada
Web-based data visualizations have become very popular for exploring data and communicating insights. Newspapers, journals, and reports regularly publish visualizations to tell compelling stories with data. Unfortunately, most visualizations are inaccessible to readers with visual impairments. For many charts on the web, there are no accompanying alternative (alt) texts, and even if such texts exist they do not adequately describe important insights from charts. To address the problem, we first interviewed 15 blind users to understand their challenges and requirements for reading data visualizations. Based on the insights from these interviews, we developed SeeChart, an interactive tool that automatically deconstructs charts from web pages and then converts them to accessible visualizations for blind people by enabling them to hear the chart summary as well as to interact through data points using the keyboard. Our evaluation with 14 blind participants suggests the efficacy of SeeChart in understanding key insights from charts and fulfilling their information needs while reducing their required time and cognitive burden.

Reverse-Engineering Visualizations: Recovering Visual Encodings from Chart Images
Jorge Poco1 and Jeffrey Heer1
We contribute an end-to-end pipeline which takes a bitmap image as input and returns a visual encoding specification as output. We present a text analysis pipeline which detects text elements in a chart, classifies their role (e.g., chart title, x-axis label, y-axis title, etc.), and recovers the text content using optical character recognition. We also train a Convolutional Neural Network for mark type classification. Using the identified text elements and graphical mark type, we can then infer the encoding specification of an input chart image. We evaluate our techniques on three chart corpora: a set of automatically labeled charts generated using Vega, charts from the Quartz news website, and charts extracted from academic papers. We demonstrate accurate automatic inference of text elements, mark types, and chart specifications across a variety of input chart types.


ChartOCR: Data Extraction from Charts Images via a Deep Hybrid Framework
Junyu Luo*1, Zekun Li 2, Jinpeng Wang3, and Chin-Yew Lin3.
Chart images are commonly used for data visualization. Automatically reading the chart values is a key step for chart content understanding. Charts have a lot of variations in style (e.g. bar chart, line chart, pie chart and etc.), which makes pure rule-based data extraction methods difficult to handle. However, it is also improper to directly apply end- to-end deep learning solutions since these methods usually deal with specific types of charts. In this paper, we propose an unified method ChartOCR to extract data from various types of charts. We show that by combing deep framework and rule-based methods, we can achieve a satisfying generalization ability and obtain accurate and semantic-rich intermediate results. Our method extracts the key points that define the chart components. By adjusting the prior rules, the framework can be applied to different chart types. Experiments show that our method achieves state-of-the- art performance with fast processing speed on two public datasets. Besides, we also introduce and evaluate on a large dataset ExcelChart400K for training deep models on chart images. The code and the dataset are publicly available at https://github.com/soap117/DeepRule

